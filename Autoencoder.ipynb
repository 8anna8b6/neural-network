{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "def relu(x):\n",
        "    \"\"\"\n",
        "    ReLU activation function.\n",
        "\n",
        "    Parameters:\n",
        "    - x: Input values.\n",
        "\n",
        "    Returns:\n",
        "    - (max(0, x)).\n",
        "    \"\"\"\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "def relu_deriv(x):\n",
        "    \"\"\"\n",
        "    Computes the derivative of the ReLU function.\n",
        "\n",
        "    Parameters:\n",
        "    - x: Input values.\n",
        "\n",
        "    Returns:\n",
        "    - Derivative of ReLU\n",
        "    \"\"\"\n",
        "    return (x > 0).astype(float)\n",
        "\n",
        "def sigmoid(x):\n",
        "    \"\"\"\n",
        "     sigmoid activation function.\n",
        "\n",
        "    Parameters:\n",
        "    - x: Input values.\n",
        "\n",
        "    Returns:\n",
        "    - Activated values using the sigmoid function.\n",
        "    \"\"\"\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def sigmoid_deriv(x):\n",
        "    \"\"\"\n",
        "    Computes the derivative of the sigmoid function.\n",
        "\n",
        "    Parameters:\n",
        "    - x: Input values.\n",
        "\n",
        "    Returns:\n",
        "    - Derivative of the sigmoid function.\n",
        "    \"\"\"\n",
        "    s = sigmoid(x)\n",
        "    return s * (1 - s)\n",
        "\n",
        "# ---------- Noise Addition Function ----------\n",
        "\n",
        "def add_noise(X, factor=0.2):\n",
        "    \"\"\"\n",
        "    Adds Gaussian noise to the input data.\n",
        "\n",
        "    Parameters:\n",
        "    - X: Original input data.\n",
        "    - factor: Noise level to apply.\n",
        "\n",
        "    Returns:\n",
        "    - Noisy version of the input data.range [0, 1].\n",
        "    \"\"\"\n",
        "    return np.clip(X + factor * np.random.randn(*X.shape), 0., 1.)"
      ],
      "metadata": {
        "id": "KxuGGORa7PQH"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Autoencoder**"
      ],
      "metadata": {
        "id": "jB-nTeF37dAu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "xsCDyVH968kJ"
      },
      "outputs": [],
      "source": [
        "class Autoencoder:\n",
        "    \"\"\"\n",
        "    A basic feedforward autoencoder for denoising input data.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, layers, lr=0.01, epochs=20, batch_size=64):\n",
        "        \"\"\"\n",
        "        Initializes the autoencoder.\n",
        "\n",
        "        Parameters:\n",
        "        - layers: A list representing the number of neurons in each layer.\n",
        "        - lr: Learning rate for training.\n",
        "        - epochs: Number of training iterations.\n",
        "        - batch_size: Number of samples in each batch.\n",
        "        \"\"\"\n",
        "        self.lr = lr\n",
        "        self.layers = layers\n",
        "        self.weights = [np.random.randn(layers[i], layers[i+1]) * 0.01 for i in range(len(layers)-1)]\n",
        "        self.biases  = [np.zeros((1, layers[i+1])) for i in range(len(layers)-1)]\n",
        "        self.epochs = epochs\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "    def _forward(self, X):\n",
        "        \"\"\"\n",
        "        Performs the forward pass through the network.\n",
        "\n",
        "        Parameters:\n",
        "        - X: Input data.\n",
        "\n",
        "        Returns:\n",
        "        - Z: List of linear outputs before activation.\n",
        "        - A: List of activations after each layer.\n",
        "        \"\"\"\n",
        "        Z, A = [], [X]\n",
        "        for i in range(len(self.weights)-1):\n",
        "            Z.append(A[-1] @ self.weights[i] + self.biases[i])\n",
        "            A.append(relu(Z[-1]))\n",
        "        Z.append(A[-1] @ self.weights[-1] + self.biases[-1])\n",
        "        A.append(sigmoid(Z[-1]))\n",
        "        return Z, A\n",
        "\n",
        "    def _backward(self, Z, A, Y):\n",
        "        \"\"\"\n",
        "        Performs the backward pass and computes gradients.\n",
        "\n",
        "        Parameters:\n",
        "        - Z: Outputs before activation from forward pass.\n",
        "        - A: Activations from forward pass.\n",
        "        - Y: Ground truth (clean) data.\n",
        "\n",
        "        Returns:\n",
        "        - grads_w: Gradients of the weights.\n",
        "        - grads_b: Gradients of the biases.\n",
        "        \"\"\"\n",
        "        grads_w, grads_b = [], []\n",
        "        delta = (A[-1] - Y) * sigmoid_deriv(Z[-1])\n",
        "        for i in reversed(range(len(self.weights))):\n",
        "            grads_w.insert(0, A[i].T @ delta)\n",
        "            grads_b.insert(0, np.sum(delta, axis=0, keepdims=True))\n",
        "            if i > 0:\n",
        "                delta = (delta @ self.weights[i].T) * relu_deriv(Z[i-1])\n",
        "        for j in range(len(self.weights)):\n",
        "            self.weights[j] -= self.lr * grads_w[j]\n",
        "            self.biases[j] -= self.lr * grads_b[j]\n",
        "\n",
        "\n",
        "    def fit(self, X_clean, X_noisy):\n",
        "        \"\"\"\n",
        "        Trains the autoencoder using mini-batch gradient descent.\n",
        "\n",
        "        Parameters:\n",
        "        - X_clean: Clean input data (ground truth).\n",
        "        - X_noisy: Noisy version of the input data.\n",
        "        \"\"\"\n",
        "        n = X_clean.shape[0]\n",
        "        for epoch in range(self.epochs):\n",
        "            idx = np.random.permutation(n)\n",
        "            X_clean, X_noisy = X_clean[idx], X_noisy[idx]\n",
        "            for i in range(0, n, self.batch_size):\n",
        "                xb, yb = X_noisy[i:i+self.batch_size], X_clean[i:i+self.batch_size]\n",
        "                Z, A = self._forward(xb)\n",
        "                self._backward(Z, A, yb)\n",
        "\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Uses the trained autoencoder to reconstruct input data.\n",
        "\n",
        "        Parameters:\n",
        "        - X: Input data to reconstruct.\n",
        "\n",
        "        Returns:\n",
        "        - The reconstructed version of the input.\n",
        "        \"\"\"\n",
        "        _, A = self._forward(X)\n",
        "        return A[-1]\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Main**"
      ],
      "metadata": {
        "id": "_S0fV1Hc7f5_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "    # Load the MNIST datasets\n",
        "    data_train = pd.read_csv(\"MNIST-train.csv\").to_numpy()\n",
        "    data_test = pd.read_csv(\"MNIST-test.csv\").to_numpy()\n",
        "\n",
        "    # Split features and labels\n",
        "    X_train, y_train = data_train[:, :-1], data_train[:, -1]\n",
        "    X_test, y_test = data_test[:, :-1], data_test[:, -1]\n",
        "\n",
        "    # Normalize pixel values\n",
        "    X_train = X_train / 255.0\n",
        "    X_test = X_test / 255.0\n",
        "\n",
        "    # Add noise\n",
        "    X_train_noisy = add_noise(X_train, 0.2)\n",
        "    X_test_noisy  = add_noise(X_test, 0.2)\n",
        "\n",
        "    # Train Autoencoder\n",
        "    autoencoder = Autoencoder(layers=[784, 128, 64, 32, 64, 128, 784], lr=0.01)\n",
        "    autoencoder.fit(X_train, X_train_noisy)\n",
        "\n",
        "    # Evaluate on test set\n",
        "    reconstructed = autoencoder.predict(X_test_noisy)\n",
        "    print(f\"\\nTest MSE: {mean_squared_error(X_test, reconstructed):.6f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zUuaLZIr7IQg",
        "outputId": "c8ac295d-d5ae-42f3-ee99-323c308d5940"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test MSE: 0.067575\n"
          ]
        }
      ]
    }
  ]
}